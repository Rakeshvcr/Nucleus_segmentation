{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Image and mask data\n",
    "class NucleusDataset(Dataset):\n",
    "    def __init__(self, path, train = True, transform=None):\n",
    "        self.path = path\n",
    "        self.train = train\n",
    "        self.transform = transform \n",
    "\n",
    "        self.IMG_WIDTH = 128\n",
    "        self.IMG_HEIGHT = 128\n",
    "        self.IMG_CHANNELS = 3\n",
    "\n",
    "\n",
    "        if train:\n",
    "            self.idx = next(os.walk(self.path))[1]\n",
    "            self.images =[]\n",
    "            self.masks = []\n",
    "\n",
    "            for id_ in tqdm(self.idx):\n",
    "                path = self.path + id_\n",
    "                img = Image.open(path + '/images/' + id_ + '.png').convert('RGB')\n",
    "                img = img.resize((self.IMG_HEIGHT, self.IMG_WIDTH))\n",
    "\n",
    "                self.images.append(img)\n",
    "\n",
    "                mask = np.zeros((self.IMG_HEIGHT, self.IMG_WIDTH, 1), dtype=np.bool)\n",
    "                for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
    "                    mask_ = Image.open(path + '/masks/' + mask_file)\n",
    "                    mask_ = np.expand_dims(mask_.resize((self.IMG_HEIGHT, self.IMG_WIDTH)), axis=-1)\n",
    "                    mask = np.maximum(mask, mask_)\n",
    "\n",
    "                    self.masks.append(mask)\n",
    "\n",
    "        else:\n",
    "            self.idx = next(os.walk(self.path))[1]\n",
    "            self.test_imgs = []\n",
    "\n",
    "            for id_ in tqdm(self.idx):\n",
    "                path = self.path + id_\n",
    "                img = Image.open(path + '/images/' + id_ + '.png').convert(\"RGB\")\n",
    "                img = img.resize((self.IMG_HEIGHT, self.IMG_WIDTH))\n",
    "\n",
    "                self.test_imgs.append(img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.train:\n",
    "            image, mask = self.images[item], self.masks[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                mask = self.transform(mask)\n",
    "            \n",
    "            return image, mask\n",
    "        \n",
    "        else:\n",
    "            image = self.test_imgs[item]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image\n",
    "            \n",
    "class Normalizer:\n",
    "    def __call__(self, image):\n",
    "        image = np.array(image)\n",
    "        image = image.astype(np.float32) / 255\n",
    "        return image\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, data):\n",
    "        if len(data.shape) == 2:\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "        elif len(data.shape) == 3:\n",
    "            data = data.transpose((2, 0, 1))\n",
    "        else:\n",
    "            print(\"Unsupported shape!\")\n",
    "        return torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 670/670 [00:32<00:00, 20.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 65/65 [00:00<00:00, 141.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# load Train and Test dataset\n",
    "train_data = NucleusDataset(path=\"nuclei_datasets/stage1_train/\",\n",
    "                                     train=True,\n",
    "                                     transform= T.Compose([Normalizer(), ToTensor()]))\n",
    "test_data = NucleusDataset(path=\"nuclei_datasets/stage1_test/\",\n",
    "                                     train=False,\n",
    "                                     transform= T.Compose([Normalizer(), ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the U-net Model\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, kernel_size=3, padding=1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.conv1_1 = nn.Conv2d(3, 16, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv1_2 = nn.Conv2d(16, 16, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2_1 = nn.Conv2d(16, 32, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv2_2 = nn.Conv2d(32, 32, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv3_2 = nn.Conv2d(64, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv4_1 = nn.Conv2d(64, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv4_2 = nn.Conv2d(128, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.maxpool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv5_1 = nn.Conv2d(128, 256, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv5_2 = nn.Conv2d(256, 256, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv5_t = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "\n",
    "        self.conv6_1 = nn.Conv2d(256, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv6_2 = nn.Conv2d(128, 128, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv6_t = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "\n",
    "        self.conv7_1 = nn.Conv2d(128, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv7_2 = nn.Conv2d(64, 64, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv7_t = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "\n",
    "        self.conv8_1 = nn.Conv2d(64, 32, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv8_2 = nn.Conv2d(32, 32, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv8_t = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
    "\n",
    "        self.conv9_1 = nn.Conv2d(32, 16, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv9_2 = nn.Conv2d(16, 16, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "        self.conv10_1 = nn.Conv2d(16, 1, kernel_size=kernel_size, padding=padding)\n",
    "        self.conv10_2 = nn.Conv2d(1, 1, kernel_size=kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = F.elu(self.conv1_1(x))\n",
    "        conv1 = F.elu(self.conv1_2(conv1))\n",
    "        pool1 = self.maxpool1(conv1)\n",
    "\n",
    "        conv2 = F.elu(self.conv2_1(pool1))\n",
    "        conv2 = F.elu(self.conv2_2(conv2))\n",
    "        pool2 = self.maxpool2(conv2)\n",
    "\n",
    "        conv3 = F.elu(self.conv3_1(pool2))\n",
    "        conv3 = F.elu(self.conv3_2(conv3))\n",
    "        pool3 = self.maxpool3(conv3)\n",
    "\n",
    "        conv4 = F.elu(self.conv4_1(pool3))\n",
    "        conv4 = F.elu(self.conv4_2(conv4))\n",
    "        pool4 = self.maxpool4(conv4)\n",
    "\n",
    "        conv5 = F.elu(self.conv5_1(pool4))\n",
    "        conv5 = F.elu(self.conv5_2(conv5))\n",
    "\n",
    "        up6 = torch.cat((self.conv5_t(conv5), conv4), dim=1)\n",
    "        conv6 = F.elu(self.conv6_1(up6))\n",
    "        conv6 = F.elu(self.conv6_2(conv6))\n",
    "\n",
    "        up7 = torch.cat((self.conv6_t(conv6), conv3), dim=1)\n",
    "        conv7 = F.elu(self.conv7_1(up7))\n",
    "        conv7 = F.relu(self.conv7_2(conv7))\n",
    "\n",
    "        up8 = torch.cat((self.conv7_t(conv7), conv2), dim=1)\n",
    "        conv8 = F.elu(self.conv8_1(up8))\n",
    "        conv8 = F.elu(self.conv8_2(conv8))\n",
    "\n",
    "        up9 = torch.cat((self.conv8_t(conv8), conv1), dim=1)\n",
    "        conv9 = F.elu(self.conv9_1(up9))\n",
    "        conv9 = F.elu(self.conv9_2(conv9))\n",
    "        \n",
    "        conv10 = F.elu(self.conv10_1(conv9))\n",
    "        \n",
    "        return torch.sigmoid(self.conv10_2(conv10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (conv1_1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2_1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_t): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv6_1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_t): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv7_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_t): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv8_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8_t): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (conv9_1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv9_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv10_1): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv10_2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# U-Net Architecture\n",
    "UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Model\n",
    "def train(train_loader, epochs, learning_rate):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {} of {}'.format(epoch + 1, epochs))\n",
    "        print('-><-' * 10)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (images, masks) in tqdm(enumerate(train_loader), total = len(train_loader)):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            loss = F.binary_cross_entropy(output, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(\"Loss: {:.4f}\\n\".format(epoch_loss))\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    torch.save(model, \"models/model_1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:06<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3594\n",
      "\n",
      "Epoch 2 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:15<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3186\n",
      "\n",
      "Epoch 3 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:10<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3167\n",
      "\n",
      "Epoch 4 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:15<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3161\n",
      "\n",
      "Epoch 5 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:19<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3165\n",
      "\n",
      "Epoch 6 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:24<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3168\n",
      "\n",
      "Epoch 7 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:25<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3163\n",
      "\n",
      "Epoch 8 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:29<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3161\n",
      "\n",
      "Epoch 9 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:38<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3160\n",
      "\n",
      "Epoch 10 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:47<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3172\n",
      "\n",
      "Epoch 11 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:43<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3162\n",
      "\n",
      "Epoch 12 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:39<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3163\n",
      "\n",
      "Epoch 13 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:35<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3162\n",
      "\n",
      "Epoch 14 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:32<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3161\n",
      "\n",
      "Epoch 15 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:45<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3162\n",
      "\n",
      "Epoch 16 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:18<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3160\n",
      "\n",
      "Epoch 17 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:13<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3163\n",
      "\n",
      "Epoch 18 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:10<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3163\n",
      "\n",
      "Epoch 19 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:10<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3161\n",
      "\n",
      "Epoch 20 of 20\n",
      "-><--><--><--><--><--><--><--><--><--><-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 84/84 [03:11<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3165\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAKESH V\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type UNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size= 8, shuffle=True)\n",
    "\n",
    "train(train_loader, epochs = 20 , learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test(test_loader , weights_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = torch.load(weights_path)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        images = next(iter(test_loader)).to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        images = tensor_to_numpy(images)\n",
    "        outputs = tensor_to_numpy(outputs)\n",
    "        show_images(images, outputs)\n",
    "\n",
    "\n",
    "def tensor_to_numpy(tensor):\n",
    "    t_numpy = tensor.cpu().numpy()\n",
    "    t_numpy = np.transpose(t_numpy, [0, 2, 3, 1])\n",
    "    t_numpy = np.squeeze(t_numpy)\n",
    "    \n",
    "    return t_numpy\n",
    "\n",
    "\n",
    "def show_images(images, masks, columns=4):\n",
    "    fig = plt.figure()\n",
    "    rows = np.ceil((images.shape[0] + masks.shape[0]) / columns)\n",
    "    index = 1\n",
    "    for image, mask in zip(images, masks):\n",
    "        f1 = fig.add_subplot(rows, columns, index)\n",
    "        f1.set_title('input')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "        f2 = fig.add_subplot(rows, columns, index)\n",
    "        f2.set_title('prediction')\n",
    "        plt.axis('off')     \n",
    "        plt.imshow(mask)\n",
    "        index += 1\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
